{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems 2,3,4,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace's inference\n",
    "\n",
    "Laplace noted that, in Paris between 1745 and 1770, the number of female to male births was 241,945 to 251,527.\n",
    "\n",
    "Let $x = 241945$ and $n=241945+251527 =493472$, noting that $\\frac{x}{n} \\approx .4903$, so that about 49% of the observed births were female.\n",
    "\n",
    "Laplace applied Bayes' Theorem and concluded that, given that one observed $x$-many female births (success) in $n$-many (ostensibly) independent Bernoulli trials with probability $\\theta$, one could infer that the probability that $\\theta>\\frac{1}{2}$ was such an\n",
    "\n",
    "> excessively small number, that one could regard it as morally certain that the differences observed in Paris between the births of males and females is due to a much greater possibility of a male birth ({cite}`Laplace1778-km` p. 431; cited in {cite}`Stigler1990-bs` p. 134).\n",
    "\n",
    "Historically, this was one of the first applications of Bayes' Theorem. This problem is mentioned and discussed in a standard text on Bayesianism, namely {cite}`Gelman2013-yi` pp. 31-32, 56, which is following {cite}`Stigler1990-bs` as far as the historical discussion goes. \n",
    "\n",
    "Problems 2,3,4,5 are my attempt to make this kind of problem more accessible to philosophers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the formal setup that we work with:\n",
    "\n",
    "- Suppose that $n,m\\geq 1$ are fixed.\n",
    "\n",
    "- Suppose that $X\\sim p_{\\theta}$, where $p_{\\theta}$ is $\\mathrm{Binom}(n,\\theta)$. \n",
    "\n",
    "- Suppose that $\\Theta= \\{\\frac{1}{m+1}, \\ldots, \\frac{m}{m+1}\\}$, so that it has $m$-elements.\n",
    "\n",
    "- Suppose that the prior is uniform, that is, $p(\\theta)=\\frac{1}{m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it a little more concrete, note that:\n",
    "\n",
    "- Suppose that $m=3$ and $n\\geq 1$ is fixed.\n",
    "\n",
    "- $\\Theta = \\{\\frac{1}{4}, \\frac{1}{2}, \\frac{3}{4}\\}$ and one has $p_{\\frac{1}{4}}\\sim \\mathrm{Binom}(n,\\frac{1}{4})$ and  $p_{\\frac{1}{2}}\\sim \\mathrm{Binom}(n,\\frac{1}{2})$ and $p_{\\frac{3}{4}}\\sim \\mathrm{Binom}(n,\\frac{3}{4})$.\n",
    "\n",
    "- Hence we are dealing with $n$-flips of either a coin heavily biased towards tails, a fair coin, or a coin biased towards heads.\n",
    "\n",
    "- In general, when $m$ is odd, the fair coin will be in the parameter space, and will be depicted with its pdf centered in a bell-like shape at $\\frac{n}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that, since the prior is uniform, one can derive the following simplified version of Bayes' Theorem:\n",
    "\n",
    "$$p(\\theta \\mid x) = \\frac{p(x\\mid \\theta)}{\\sum_{\\theta\\in \\Theta} p(x\\mid \\theta)}$$\n",
    "\n",
    "Your answer should just be a proof with 1-2 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace described this version of Bayes' Theorem as follows:\n",
    "\n",
    "> If an event can be produced by a number of different causes, then the probabilities of these causes  given the event  are to each other as the probabilities of the event  given the causes , and the probability of the existence of each of these is equal to the probability of the event given the cause, divided by sum of each of these causes ({cite}`Laplace1774-st`, translation from {cite}`Stigler1990-bs` p. 102).\n",
    "\n",
    "Rewrite this quotation word-for-word but include the mathematical expressions $\\theta$, $\\Theta$, $x$, $p(x\\mid \\theta)$, $p(\\theta\\mid x)$, and $\\sum_{\\theta\\in \\Theta} p(x\\mid \\theta)$ where appropriate. The aim is simply to make sure that we see the correspondence between the mathematical formlism of Bayes' theorem (in the case of the uniform prior) and what its original developers thought it meant: they were clearly thinking of the elements of the parameter space as partially indicative of causes which reveal their effects through the sample space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before proceeding further\n",
    "\n",
    "Before proceeding further, please follow the following instructions:\n",
    "\n",
    "1. Click on the <i class=\"fa fa-rocket\" aria-hidden=\"true\"></i> icon at the top center-right, which will launch the page in a binder (experience suggests that Firefox and Chrome work best)\n",
    "\n",
    "2. After it loads (it takes about 3-5 minutes), use the menu bars to access:\n",
    "\n",
    "- View | Collapse All Code\n",
    "\n",
    "- Run | Run All Cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "The below graph to the left shows two pieces of information:\n",
    "\n",
    "- For each element $\\theta$ of the parameter space, it displays its pdf, drawn in different colored lines. If a colored line is associated to $\\theta$, then for a value $x$ on the $x$-axis, the value of the colored line answers the question: if the random variable were distributed according to $p_{\\theta}$, then how probable it is that the random variable would take value $x$? Since we are dealing with Bernoulli distributions $\\mathrm{Binom}(n,\\theta)$, the possible values of $x$ are $0,1,2,\\ldots, n$, and they should be clustered in a bell like shape around $\\theta$. \n",
    "\n",
    "- The dotted purple line displays the evidence, that is $p(x)=\\sum_{\\theta\\in \\Theta} p(\\theta)\\cdot p(\\theta\\mid x)$. \n",
    "\n",
    "The graph in the center displays three pieces of interlinked information:\n",
    "\n",
    "- For each $x$ on the $x$-axis, it displays immediately above it the values of the posterior $p(\\theta\\mid x)$, as $\\theta$ ranges over the parameter space. Note that the values right above each $x$ thus sum to one. However, when one focuses on a fixed colored line associated to a specific parameter $\\theta$ and follows it from left to right as the values of $x$ get bigger, one is considering how the posterior $p(\\theta\\mid x)$ changes as a function of $x$.\n",
    "\n",
    "- It allows you to select a specific value of $x$ and focus on what is directly above it, marked with a dotted black line. This allows one to hone in the posterior $p(\\theta\\mid x)$ as $x$ is fixed and $\\theta$ is allowed to vary.\n",
    "\n",
    "- For the value of $x$ which you chose, it displays the top two highest values of $p(\\theta\\mid x)$ by building a horizontal line that takes one from this value over to the $y$-axis, so you can quickly tell by looking what these top two values are. The top value is in gold, and the second top value is in silver.\n",
    "\n",
    "The graph to the right displays one piece of information:\n",
    "\n",
    "- In addition to the $x$ that you already chose, it allows you to vary the choice of the parameter $\\iota=\\frac{i+1}{m+1}$ for $i=0, \\ldots, m-1$, by varying $i$: as you choose $i$ closer to $m$ you are choosing a parameter with a higher bias towards heads. The graph then displays the value $\\sum_{\\theta> \\iota} p(\\theta\\mid x)$, which is the probability, conditional on having observed that the random variable took value $x$, that the parameter is $>\\iota$, which we hence abbreviate as $Pr(\\theta> \\iota\\mid x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import binom\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt   \n",
    "import metakernel\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import ipywidgets as widgets    \n",
    "from ipywidgets import interact, interactive, fixed, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def find_two_largest_indices(list_of_lists, j):\n",
    "    # Convert the column at index j to a numpy array\n",
    "    column = np.array([row[j] for row in list_of_lists])\n",
    "\n",
    "    # Find the indices of the two largest values\n",
    "    indices = np.argpartition(column, -2)[-2:]\n",
    "\n",
    "    # Sort the indices so that the first index corresponds to the largest value\n",
    "    # and the second index corresponds to the second largest value\n",
    "    indices = indices[np.argsort(column[indices])][::-1]\n",
    "\n",
    "    return indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# size_sample_space, size_parameter_space are positive integers\n",
    "# prior is list of length size_parameter_space of non-negative numbers that sum to 1\n",
    "# likelihood is a list of length size_parameter_space \n",
    "# where each element is a list of length size_sample_space numbers that sum to 1\n",
    "\n",
    "def bayes(size_sample_space,size_parameter_space,prior,likelihood, xlim0, xlim1, observed_value = None, Î¹ = None):\n",
    "\n",
    "    sample_space = list(range(size_sample_space))\n",
    "\n",
    "    parameter_space = list(range(size_parameter_space))\n",
    "\n",
    "\n",
    "    max_value = max(max(sublist) for sublist in likelihood)\n",
    "\n",
    "\n",
    "    evidence =  []\n",
    "\n",
    "    for j in sample_space:\n",
    "        evidence.append(sum([prior[i]*likelihood[i][j] for i in parameter_space]))\n",
    "\n",
    "\n",
    "\n",
    "    posterior = [[None]*size_sample_space for _ in range(size_parameter_space)]\n",
    "\n",
    "    posteriorupvalue= [[None]*size_parameter_space for _ in range(size_sample_space)]\n",
    "\n",
    "    for i in parameter_space:\n",
    "        for j in sample_space:\n",
    "            posterior[i][j] = ((likelihood[i][j]*prior[i]) / (evidence[j]+ 1e-10))\n",
    "\n",
    "    for j in sample_space:\n",
    "        for i in parameter_space:\n",
    "            posteriorupvalue[j][i] = sum([posterior[i1][j] for i1 in parameter_space if i1 > i])\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 5))  # 1 row, 3 columns\n",
    "\n",
    "\n",
    "\n",
    "    for i in parameter_space:\n",
    "        line, = axs[0].plot(sample_space, likelihood[i])\n",
    "    axs[0].plot(sample_space, evidence, '--', label='evidence', color='purple')        \n",
    "    axs[0].set_ylim(0, max_value+max_value/10)\n",
    "    axs[0].set_xlim(0, xlim0)    \n",
    "    axs[0].set_title('likelihood p(x | $\\\\theta$)')\n",
    "    axs[0].legend()\n",
    "    axs[0].legend(loc='upper right')\n",
    "    axs[0].set_xticks(np.round(np.linspace(0, xlim1, num=11), 0))\n",
    "    axs[0].tick_params(axis='x', labelsize=5)          \n",
    "\n",
    "    for i in parameter_space:\n",
    "        axs[1].plot(sample_space, posterior[i]) \n",
    "    if observed_value != None:\n",
    "        axs[1].axvline(x=observed_value, color='black', linestyle='--', label='x')            \n",
    "        indices = find_two_largest_indices(posterior, observed_value)\n",
    "        axs[1].axhline(y=posterior[indices[0]][observed_value], color='#ffd700', linestyle='--', label='1st')  \n",
    "        axs[1].axhline(y=posterior[indices[1]][observed_value], color='#C0C0C0', linestyle='--', label='2nd')  \n",
    "    axs[1].legend()\n",
    "    axs[1].legend(loc='upper right')\n",
    "\n",
    "    axs[1].set_ylim(0, 1.1)\n",
    "    axs[1].set_title('posterior p($\\\\theta$  | x)')\n",
    "    axs[1].set_xlim(0, xlim1)    \n",
    "    axs[1].set_yticks([0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1])\n",
    "    axs[1].set_xticks(np.round(np.linspace(0, xlim1, num=11), 0))  \n",
    "    axs[1].tick_params(axis='x', labelsize=5)    \n",
    "\n",
    "    if Î¹ != None:\n",
    "        axs[2].axvline(x=Î¹, color='g', linestyle='--', label='Î¹') \n",
    "        axs[2].axhline(y=posteriorupvalue[observed_value][Î¹], color='red', linestyle='--')               \n",
    "    axs[2].plot(parameter_space, posteriorupvalue[observed_value]) \n",
    "    axs[2].set_title('Pr($\\\\theta> \\\\iota$  | x), $\\\\iota$ is Binom(%i, %1.2f)' % (size_sample_space-1, (Î¹+1)/(size_parameter_space+1))) \n",
    "    axs[2].set_yticks([0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1])    \n",
    "    axs[2].legend()\n",
    "    axs[2].set_ylim(0, 1.1)    \n",
    "    axs[2].legend(loc='center right')    \n",
    "\n",
    "    return posterior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def bayes_binomials(n,m, prior, observed_value = None, Î¹=None):\n",
    "\n",
    "    my_sample_space = np.arange(0, n+1)\n",
    "\n",
    "    my_parameter_space = np.arange(0, m)\n",
    "\n",
    "    my_likelihood = [[None]*m for _ in range(m)]\n",
    "\n",
    "    for i in range(m):\n",
    "\n",
    "        my_likelihood[i] = binom.pmf(my_sample_space, n, (i+1)/(m+1))\n",
    "\n",
    "    bayes(n+1,m,prior,my_likelihood, n, n, observed_value, Î¹)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def bayes_binomials_uniform_prior(n,m, x = None, Î¹ = None):\n",
    "\n",
    "    prior = [1/(m+1)]*(m)\n",
    "\n",
    "    bayes_binomials(n,m,prior, x, Î¹)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1766d51653c34d14ace6c22d7445effa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='n', max=5000, min=1), IntSlider(value=9, description='â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.bayes_binomials_uniform_prior(n, m, x=None, Î¹=None)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_x_range(*args):\n",
    "    x_slider.max = n_slider.value\n",
    "\n",
    "def update_Î¹_range(*args):\n",
    "    Î¹_slider.max = m_slider.value - 1\n",
    "\n",
    "n_slider = widgets.IntSlider(min=1, max=5000, step=1, value=100)\n",
    "m_slider = widgets.IntSlider(min=1, max=99, step=1, value=9)\n",
    "x_slider = widgets.IntSlider(min=1, max=n_slider.value, step=1, value=40)\n",
    "Î¹_slider = widgets.IntSlider(min=1, max=m_slider.value - 1, step=0, value=3)\n",
    "\n",
    "n_slider.observe(update_x_range, 'value')\n",
    "m_slider.observe(update_Î¹_range, 'value')\n",
    "\n",
    "interact(bayes_binomials_uniform_prior, n=n_slider, m=m_slider, x=x_slider, Î¹=Î¹_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions in 1-2 complete English sentences each:\n",
    "\n",
    "1. Why does increasing the value of $n$ make the bell-like shapes of the posterior change into more rectangle-like shapes? \n",
    "\n",
    "2. Why does increasing the value of $m$ lower the posterior? \n",
    "\n",
    "3. For small values of $n,m$ (say $n=100$ and $m=9$), as one increases the value of $x$ the difference between the gold and silver horizontal lines fluctuates. Why is this? \n",
    "\n",
    "(Note: you will only be able to change the values once you moved to the binder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "For each of the following three settings, write down $Pr(\\theta>\\iota\\mid x)$ and $Binom(n,\\theta)$ from the rightmost graph:\n",
    "\n",
    "1. $n=500$, $m=9$, $x=244$ (approximately 49% of 500), and $\\iota = 4$.\n",
    "\n",
    "2. $n=5000$, $m=9$, $x=2450$ (approximately 49% of 5000), and $\\iota = 4$.\n",
    "\n",
    "3. $n=5000$, $m=99$, $x=2450$ (approximately 49% of 5000), and $\\iota = 49$.\n",
    "\n",
    "In a paragraph (4-5 sentences), describe whether this on the whole supports Laplace's conclusion or not.\n",
    "\n",
    "*Note 1*: we used $n=5000$ as our maximum because if we tried Laplace's actual number our machine would take about 5-10 minutes to do it. If you want to try just type the following in a new cell and press shift return and wait 5-10 minutes (note that if you do this, the above graph is going to become non-responsive):\n",
    "\n",
    "```\n",
    "bayes_binomials_uniform_prior(241945+251527, 99, 241945, 49)\n",
    "```\n",
    "\n",
    "*Note 2*: For all these numbers $n,m,x,\\iota$, the toggles might only allow you to get within a small range of these values; don't worry about that, just get as close as you can.\n",
    "\n",
    "*Note 3*: note that\n",
    "\n",
    "- the value $Pr(\\theta>\\iota\\mid x)$ is the place where the red line meets the $y$-axis; just eyeball it.\n",
    "\n",
    "- the values of $n,\\theta$ are displayed in the title $\\iota \\sim Binom(n,\\theta)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Laplace's inference can be compactly expressed as follows:\n",
    "\n",
    "1. Since 49% of observed births in a large sample are female, it is morally certain that it there is a much greater possibility of male birth.\n",
    "\n",
    "Many of our inferences are contrastive in nature. Consider the following variations:\n",
    "\n",
    "2. Since 49% of observed births in a large sample  are female, it is significantly more plausible that there is much greater possibility of male birth than that our data was generated by a fluke of measurement. \n",
    "\n",
    "3. Since 49% of observed births in a large sample  are female, it is significantly more plausible that there is much greater possibility of male birth than that there is a much greater possibility of female birth.\n",
    "\n",
    "4. Since 49% of *observed* births in a large sample  are female, it is significantly plausible that 49% of *unobserved* births are female. \n",
    "\n",
    "In a paragraph (4-5 sentences), indicate whether you think that 2, 3, or 4 (and if so which one) is a good representation of Laplace's inference in 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
