%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Sean Walsh at 2024-05-04 20:41:53 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@article{Laplace1774-st,
	author = {Laplace, Pierre Simon},
	date-added = {2024-05-04 20:41:21 -0700},
	date-modified = {2024-05-04 20:41:21 -0700},
	journal = {M{\'e}morie de l'Academie royale des sciences present{\'e}s par divers savans},
	pages = {621--656},
	title = {M{\'e}morie sur les probabilit{\'e} des causes par les {\'e}v{\`e}nemens},
	volume = 6,
	year = 1774}

@article{Laplace1778-km,
	author = {Laplace, Pierre Simon},
	date-added = {2024-05-04 20:33:01 -0700},
	date-modified = {2024-05-04 20:41:50 -0700},
	journal = {M{\'e}morie de l'Academie royale des sciences present{\'e}s par divers savans},
	pages = {227-332},
	title = {M{\'e}morie sur les probabilit{\'e}s},
	year = {1778}}

@book{Stigler1990-bs,
	abstract = {This magnificent book is the first comprehensive history of
               statistics from its beginnings around 1700 to its emergence as a
               distinct and mature discipline around 1900. Stephen M. Stigler
               shows how statistics arose from the interplay of mathematical
               concepts and the needs of several applied sciences including
               astronomy, geodesy, experimental psychology, genetics, and
               sociology. He addresses many intriguing questions: How did
               scientists learn to combine measurements made under different
               conditions? And how were they led to use probability theory to
               measure the accuracy of the result? Why were statistical methods
               used successfully in astronomy long before they began to play a
               significant role in the social sciences? How could the
               introduction of least squares predate the discovery of
               regression by more than eighty years? On what grounds can the
               major works of men such as Bernoulli, De Moivre, Bayes,
               Quetelet, and Lexis be considered partial failures, while those
               of Laplace, Galton, Edgeworth, Pearson, and Yule are counted as
               successes? How did Galton's probability machine (the quincunx)
               provide him with the key to the major advance of the last half
               of the nineteenth century?Stigler's emphasis is upon how, when,
               and where the methods of probability theory were developed for
               measuring uncertainty in experimental and observational science,
               for reducing uncertainty, and as a conceptual framework for
               quantitative studies in the social sciences. He describes with
               care the scientific context in which the different methods
               evolved and identifies the problems (conceptual or mathematical)
               that retarded the growth of mathematical statistics and the
               conceptual developments that permitted major
               breakthroughs.Statisticians, historians of science, and social
               and behavioral scientists will gain from this book a deeper
               understanding of the use of statistical methods and a better
               grasp of the promise and limitations of such techniques. The
               product of ten years of research, The History of Statistics will
               appeal to all who are interested in the humanistic study of
               science.},
	author = {Stigler, Stephen M},
	date-added = {2024-05-04 20:28:10 -0700},
	date-modified = {2024-05-04 20:28:10 -0700},
	language = {en},
	month = mar,
	publisher = {Harvard University Press},
	title = {The History of Statistics: The Measurement of Uncertainty before 1900},
	year = 1990}

@book{Gelman2013-yi,
	author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
	date-added = {2024-05-04 20:24:47 -0700},
	date-modified = {2024-05-04 20:24:47 -0700},
	edition = 3,
	language = {en},
	month = nov,
	publisher = {Chapman and Hall/CRC},
	title = {Bayesian Data Analysis},
	year = 2013}

@article{Neyman1933-zd,
	abstract = {The problem of testing statistical hypotheses is an old one. Its
               origin is usually connected with the name of Thomas Bayes, who
               gave the well-known theorem on the probabilities a posteriori of
               the possible ``causes`` of a given event. Since then it has been
               discussed by many writers of whom we shall here mention two
               only, Bertrand and Borel, whose differing views serve well to
               illustrate the point from which we shall approach the subject.
               Bertrand put into statistical form a variety of hypotheses, as
               for example the hypothesis that a given group of stars with
               relatively small angular distances between them as seen from the
               earth, form a ``system'' or group in space. His method of
               attack, which is that in common use, consisted essentially in
               calculating the probability, P, that a certain character, x , of
               the observed facts would arise if the hypothesis tested were
               true. If P were very small, this would generally be considered
               as an indication that the hypothesis, H, was probably false, and
               vice versa . Bertrand expressed the pessimistic view that no
               test of this kind could give reliable results. Borel, however,
               in a later discussion, considered that the method described
               could be applied with success provided that the character, x ,
               of the observed facts were properly chosen---were, in fact, a
               character which he terms ``en quelque sorte remarquable.''},
	author = {Neyman, J and Pearson, E S},
	date-added = {2024-04-22 18:57:55 -0700},
	date-modified = {2024-04-22 18:58:28 -0700},
	journal = {Philosophical transactions of the Royal Society of London},
	number = {694-706},
	pages = {289--337},
	title = {On the problem of the most efficient tests of statistical hypotheses},
	volume = 231,
	year = 1933}

@book{Chihara2018-xh,
	abstract = {This thoroughly updated second edition combines the latest
               software applications with the benefits of modern resampling
               techniques Resampling helps students understand the meaning of
               sampling distributions, sampling variability, P-values,
               hypothesis tests, and confidence intervals. The second edition
               of Mathematical Statistics with Resampling and R combines modern
               resampling techniques and mathematical statistics. This book has
               been classroom-tested to ensure an accessible presentation, uses
               the powerful and flexible computer language R for data analysis
               and explores the benefits of modern resampling techniques. This
               book offers an introduction to permutation tests and bootstrap
               methods that can serve to motivate classical inference methods.
               The book strikes a balance between theory, computing, and
               applications, and the new edition explores additional topics
               including consulting, paired t test, ANOVA and Google Interview
               Questions. Throughout the book, new and updated case studies are
               included representing a diverse range of subjects such as flight
               delays, birth weights of babies, and telephone company repair
               times. These illustrate the relevance of the real-world
               applications of the material. This new edition: * Puts the focus
               on statistical consulting that emphasizes giving a client an
               understanding of data and goes beyond typical expectations *
               Presents new material on topics such as the paired t test,
               Fisher's Exact Test and the EM algorithm * Offers a new section
               on ``Google Interview Questions'' that illustrates statistical
               thinking * Provides a new chapter on ANOVA * Contains more
               exercises and updated case studies, data sets, and R code
               Written for undergraduate students in a mathematical statistics
               course as well as practitioners and researchers, the second
               edition of Mathematical Statistics with Resampling and R
               presents a revised and updated guide for applying the most
               current resampling techniques to mathematical statistics.},
	author = {Chihara, Laura M and Hesterberg, Tim C},
	date-added = {2024-04-18 00:58:22 -0700},
	date-modified = {2024-04-18 00:58:22 -0700},
	language = {en},
	month = sep,
	publisher = {John Wiley \& Sons},
	title = {Mathematical Statistics with Resampling and {R}},
	year = 2018}

@book{Melia2014-vh,
	abstract = {This introduction to modality places the emphasis on the
                 metaphysics of modality rather than on the formal semetics of
                 quantified modal logic. The text begins by introducing
                 students to the ``de re/de dicto'' distinction,
                 conventionalist and conceptualist theories of modality and
                 some of the key problems in modality, particularly Quine's
                 criticisms. It then moves on to explain how possible worlds
                 provide a solution to many of the problems in modality and how
                 possible worlds themselves have been used to analyse notions
                 outside modality such as properties and propositions. Possible
                 worlds introduce problems of their own and the book argues
                 that to make progress with these problems a theory of possible
                 worlds is required. The pros and cons of various theories of
                 possible worlds are then examined in turn, including those of
                 Lewis, Kripke, Adams, Stalnaker and Plantinga.},
	author = {Melia, Joseph},
	date-added = {2024-04-18 00:46:10 -0700},
	date-modified = {2024-04-18 00:46:10 -0700},
	language = {en},
	month = dec,
	original_id = {2bb808e5-c425-00ab-8cf0-0e29575c136c},
	publisher = {Routledge},
	title = {Modality},
	year = 2014}

@article{Efron1979-ux,
	abstract = {We discuss the following problem: given a random sample
               $\mathbf\{X\} = (X_1, X_2, \cdots, X_n)$ from an unknown
               probability distribution $F$, estimate the sampling distribution
               of some prespecified random variable $R(\mathbf\{X\}, F)$, on
               the basis of the observed data $\mathbf\{x\}$. (Standard
               jackknife theory gives an approximate mean and variance in the
               case $R(\mathbf\{X\}, F) = \theta(\hat\{F\}) - \theta(F),
               \theta$ some parameter of interest.) A general method, called
               the ``bootstrap,'' is introduced, and shown to work
               satisfactorily on a variety of estimation problems. The
               jackknife is shown to be a linear approximation method for the
               bootstrap. The exposition proceeds by a series of examples:
               variance of the sample median, error rates in a linear
               discriminant analysis, ratio estimation, estimating regression
               parameters, etc.},
	author = {Efron, B},
	date-added = {2024-04-18 00:42:35 -0700},
	date-modified = {2024-04-18 00:42:35 -0700},
	journal = {aos},
	keywords = {62G05; 62G15; 62H30; 62J05; bootstrap; discriminant analysis; error rate estimation; jackknife; Nonlinear regression; nonparametric variance estimation; Resampling; subsample values;},
	language = {en},
	month = jan,
	number = 1,
	pages = {1--26},
	publisher = {Institute of Mathematical Statistics},
	title = {Bootstrap Methods: Another Look at the Jackknife},
	volume = 7,
	year = 1979}

@book{Efron1982-ck,
	abstract = {{\ldots} Resampling procedures .................................... {\ldots}
               Relation between the jackknife and bootstrap estimates of
               standard {\ldots} which included several other talks on jackknife -
               bootstrap related topics{\ldots}},
	author = {Efron, Bradley},
	date-added = {2024-04-18 00:41:49 -0700},
	date-modified = {2024-04-18 00:41:49 -0700},
	publisher = {SIAM},
	title = {The jackknife, the bootstrap and other resampling plans},
	year = 1982}

@book{Stigler2016-lx,
	abstract = {What gives statistics its unity as a science? Stephen Stigler
               sets forth the seven foundational ideas of statistics---a
               scientific discipline related to but distinct from mathematics
               and computer science.Even the most basic idea---aggregation,
               exemplified by averaging---is counterintuitive. It allows one to
               gain information by discarding information, namely, the
               individuality of the observations. Stigler's second pillar,
               information measurement, challenges the importance of ``big
               data'' by noting that observations are not all equally
               important: the amount of information in a data set is often
               proportional to only the square root of the number of
               observations, not the absolute number. The third idea is
               likelihood, the calibration of inferences with the use of
               probability. Intercomparison is the principle that statistical
               comparisons do not need to be made with respect to an external
               standard. The fifth pillar is regression, both a paradox (tall
               parents on average produce shorter children; tall children on
               average have shorter parents) and the basis of inference,
               including Bayesian inference and causal reasoning. The sixth
               concept captures the importance of experimental design---for
               example, by recognizing the gains to be had from a combinatorial
               approach with rigorous randomization. The seventh idea is the
               residual: the notion that a complicated phenomenon can be
               simplified by subtracting the effect of known causes, leaving a
               residual phenomenon that can be explained more easily.The Seven
               Pillars of Statistical Wisdom presents an original, unified
               account of statistical science that will fascinate the
               interested layperson and engage the professional statistician.},
	author = {Stigler, Stephen M},
	date-added = {2024-04-15 20:23:15 -0700},
	date-modified = {2024-04-15 20:23:15 -0700},
	language = {en},
	month = mar,
	publisher = {Harvard University Press},
	title = {The Seven Pillars of Statistical Wisdom},
	year = 2016}

@book{Wasserman2013-bc,
	abstract = {Taken literally, the title ``All of Statistics'' is an
               exaggeration. But in spirit, the title is apt, as the book does
               cover a much broader range of topics than a typical introductory
               book on mathematical statistics. This book is for people who
               want to learn probability and statistics quickly. It is suitable
               for graduate or advanced undergraduate students in computer
               science, mathematics, statistics, and related disciplines. The
               book includes modern topics like nonparametric curve estimation,
               bootstrapping, and clas sification, topics that are usually
               relegated to follow-up courses. The reader is presumed to know
               calculus and a little linear algebra. No previous knowledge of
               probability and statistics is required. Statistics, data mining,
               and machine learning are all concerned with collecting and
               analyzing data. For some time, statistics research was con
               ducted in statistics departments while data mining and machine
               learning re search was conducted in computer science
               departments. Statisticians thought that computer scientists were
               reinventing the wheel. Computer scientists thought that
               statistical theory didn't apply to their problems. Things are
               changing. Statisticians now recognize that computer scientists
               are making novel contributions while computer scientists now
               recognize the generality of statistical theory and methodology.
               Clever data mining algo rithms are more scalable than
               statisticians ever thought possible. Formal sta tistical theory
               is more pervasive than computer scientists had realized.},
	author = {Wasserman, Larry},
	date-added = {2024-04-15 20:06:49 -0700},
	date-modified = {2024-04-15 20:06:49 -0700},
	language = {en},
	month = dec,
	publisher = {Springer Science \& Business Media},
	title = {All of Statistics: A Concise Course in Statistical Inference},
	year = 2013}

@book{Tijms2012-ot,
	abstract = {Understanding Probability is a unique and stimulating approach
               to a first course in probability. The first part of the book
               demystifies probability and uses many wonderful probability
               applications from everyday life to help the reader develop a
               feel for probabilities. The second part, covering a wide range
               of topics, teaches clearly and simply the basics of probability.
               This fully revised third edition has been packed with even more
               exercises and examples and it includes new sections on Bayesian
               inference, Markov chain Monte-Carlo simulation, hitting
               probabilities in random walks and Brownian motion, and a new
               chapter on continuous-time Markov chains with applications. Here
               you will find all the material taught in an introductory
               probability course. The first part of the book, with its
               easy-going style, can be read by anybody with a reasonable
               background in high school mathematics. The second part of the
               book requires a basic course in calculus.},
	author = {Tijms, Henk},
	date-added = {2024-04-13 17:01:19 -0700},
	date-modified = {2024-04-13 17:01:19 -0700},
	language = {en},
	month = jun,
	publisher = {Cambridge University Press},
	title = {Understanding Probability},
	year = 2012}

@book{Skiena2017-ab,
	author = {Skiena, Steven S},
	date-added = {2024-04-13 15:00:20 -0700},
	date-modified = {2024-04-13 15:00:48 -0700},
	publisher = {Springer},
	title = {The Data Science Design Manual},
	year = {2017}}

@book{Wittgenstein1922-ed,
	address = {London},
	author = {Wittgenstein, Ludwig},
	date-added = {2024-04-13 11:24:21 -0700},
	date-modified = {2024-04-13 11:24:21 -0700},
	publisher = {Routledge and Kegan Paul},
	title = {Tractatus {Logico-Philosophicus}},
	year = 1922}

@book{Glymour1980-mn,
	abstract = {The Description for this book, Theory and Evidence, will be
               forthcoming.},
	author = {Glymour, Clark N},
	date-added = {2024-04-12 22:34:56 -0700},
	date-modified = {2024-04-12 22:34:56 -0700},
	language = {en},
	publisher = {Princeton University Press},
	title = {Theory and Evidence},
	year = 1980}

@article{De_Finetti1964-rr,
	abstract = {Henri Poincar{\'e}, the immortal scientist whose name this
               institute honors, and who brought to life with his ingenious
               ideas so many branches of mathematics, is without doubt also the
               thinker who attributed the greatest domain of application to the
               theory of probability and {\ldots}},
	author = {De Finetti, Bruno},
	date-added = {2024-04-12 22:30:33 -0700},
	date-modified = {2024-04-12 22:30:33 -0700},
	journal = {Studies in subjective probability},
	pages = {94--158},
	publisher = {Wiley New York},
	title = {Foresight: Its logical laws, its subjective sources},
	volume = 1964,
	year = 1964}

@book{Von_Mises1957-rk,
	address = {New York},
	author = {von Mises, Richard},
	date-added = {2024-04-12 22:26:46 -0700},
	date-modified = {2024-04-12 22:26:46 -0700},
	publisher = {Macmillan},
	title = {Probability, {S}tatistics and {T}ruth},
	year = 1957}

@book{Dale1995-zf,
	author = {Dale, Andrew I and Laplace, Pierre-Simon},
	date-added = {2024-04-12 22:24:27 -0700},
	date-modified = {2024-04-12 22:24:27 -0700},
	publisher = {Springer},
	title = {{Pierre-Simon} Laplace Philosophical Essay on Probabilities},
	year = 1995}

@book{Stalnaker1987-fw,
	abstract = {The abstract structure of inquiry - the process of acquiring and
               changing beliefs about the world - is the focus of this book
               which takes the position that the ``pragmatic'' rather than the
               ``linguistic'' approach better solves the philosophical problems
               about the nature of mental representation, and better accounts
               for the phenomena of thought and speech. It discusses
               propositions and propositional attitudes (the cluster of
               activities that constitute inquiry) in general and takes up the
               way beliefs change in response to potential new information,
               suggesting that conditional propositions should be understood as
               projections of epistemic policies onto the world.A Bradford
               Book.},
	author = {Stalnaker, Robert C},
	date-added = {2024-04-12 21:57:01 -0700},
	date-modified = {2024-04-12 21:57:01 -0700},
	language = {en},
	month = mar,
	publisher = {MIT Press},
	title = {Inquiry},
	year = 1987}

@book{Schervish2012-cn,
	abstract = {The aim of this graduate textbook is to provide a comprehensive
               advanced course in the theory of statistics covering those
               topics in estimation, testing, and large sample theory which a
               graduate student might typically need to learn as preparation
               for work on a Ph.D. An important strength of this book is that
               it provides a mathematically rigorous and even-handed account of
               both Classical and Bayesian inference in order to give readers a
               broad perspective. For example, the ``uniformly most powerful''
               approach to testing is contrasted with available
               decision-theoretic approaches.},
	author = {Schervish, Mark J},
	date-added = {2024-04-12 09:15:20 -0700},
	date-modified = {2024-04-12 09:15:25 -0700},
	language = {en},
	month = dec,
	publisher = {Springer},
	title = {Theory of Statistics},
	year = 2012}

@book{Gabbay2011-ll,
	abstract = {Statisticians and philosophers of science have many common
               interests but restricted communication with each other. This
               volume aims to remedy these shortcomings. It provides
               state-of-the-art research in the area of philosophy of
               statistics by encouraging numerous experts to communicate with
               one another without feeling ``restricted by their disciplines or
               thinking ``piecemeal in their treatment of issues. A second goal
               of this book is to present work in the field without bias toward
               any particular statistical paradigm. Broadly speaking, the
               essays in this Handbook are concerned with problems of
               induction, statistics and probability. For centuries,
               foundational problems like induction have been among
               philosophers' favorite topics; recently, however,
               non-philosophers have increasingly taken a keen interest in
               these issues. This volume accordingly contains papers by both
               philosophers and non-philosophers, including scholars from nine
               academic disciplines. Provides a bridge between philosophy and
               current scientific findings Covers theory and applications
               Encourages multi-disciplinary dialogue},
	author = {Gabbay, D M and Thagard, P and Woods, J and Bandyopadhyay, P S and {others}},
	date-added = {2024-04-12 09:15:01 -0700},
	date-modified = {2024-04-12 09:15:01 -0700},
	language = {en},
	month = may,
	publisher = {Elsevier},
	title = {Philosophy of Statistics},
	year = 2011}

@book{DeGroot2011-we,
	author = {DeGroot, Morris H and Schervish, Mark J},
	date-added = {2024-04-12 09:14:57 -0700},
	date-modified = {2024-04-12 22:47:18 -0700},
	edition = 4,
	month = jan,
	publisher = {Pearson},
	title = {Probability and Statistics},
	year = 2011}

@book{Williamson2010-dq,
	abstract = {How strongly should you believe the various propositions that
               you can express? That is the key question facing Bayesian
               epistemology. Subjective Bayesians hold that it is largely
               (though not entirely) up to the agent as to which degrees of
               belief to adopt. Objective Bayesians, on the other hand,
               maintain that appropriate degrees of belief are largely (though
               not entirely) determined by the agent's evidence. This book
               states and defends a version of objective Bayesian epistemology.
               According to this version, objective Bayesianism is
               characterized by three norms: · Probability - degrees of belief
               should be probabilities · Calibration - they should be
               calibrated with evidence · Equivocation - they should otherwise
               equivocate between basic outcomes Objective Bayesianism has been
               challenged on a number of different fronts. For example, some
               claim it is poorly motivated, or fails to handle qualitative
               evidence, or yields counter-intuitive degrees of belief after
               updating, or suffers from a failure to learn from experience. It
               has also been accused of being computationally intractable,
               susceptible to paradox, language dependent, and of not being
               objective enough. Especially suitable for graduates or
               researchers in philosophy of science, foundations of statistics
               and artificial intelligence, the book argues that these
               criticisms can be met and that objective Bayesianism is a
               promising theory with an exciting agenda for further research.},
	author = {Williamson, Jon},
	date-added = {2024-04-12 09:13:50 -0700},
	date-modified = {2024-04-12 09:13:50 -0700},
	language = {en},
	month = may,
	publisher = {Oxford University Press},
	title = {In Defence of Objective Bayesianism},
	year = 2010}

@book{Royall2017-gc,
	abstract = {Interpreting statistical data as evidence, Statistical
                 Evidence: A Likelihood Paradigm focuses on the law of
                 likelihood, fundamental to solving many of the problems
                 associated with interpreting data in this way. Statistics has
                 long neglected this principle, resulting in a seriously
                 defective methodology. This book redresses the balance,
                 explaining why science has clung to a defective methodology
                 despite its well-known defects. After examining the strengths
                 and weaknesses of the work of Neyman and Pearson and the
                 Fisher paradigm, the author proposes an alternative paradigm
                 which provides, in the law of likelihood, the explicit concept
                 of evidence missing from the other paradigms. At the same
                 time, this new paradigm retains the elements of objective
                 measurement and control of the frequency of misleading
                 results, features which made the old paradigms so important to
                 science. The likelihood paradigm leads to statistical methods
                 that have a compelling rationale and an elegant simplicity, no
                 longer forcing the reader to choose between frequentist and
                 Bayesian statistics.},
	author = {Royall, Richard},
	date-added = {2024-04-12 09:13:42 -0700},
	date-modified = {2024-04-12 09:13:42 -0700},
	language = {en},
	month = nov,
	original_id = {d69aa4c7-3667-0446-abb7-ac101969e597},
	publisher = {Routledge},
	title = {Statistical Evidence: A Likelihood Paradigm},
	year = 2017}

@incollection{Sprenger2016-xc,
	abstract = {Abstract. Bayesianism and frequentism are the two grand schools
               of statistical inference, divided by fundamentally different
               philosophical assumptions and},
	author = {Sprenger, Jan},
	booktitle = {The Oxford Handbook of Probability and Philosophy},
	date-added = {2024-04-12 09:13:31 -0700},
	date-modified = {2024-04-12 09:13:31 -0700},
	pages = {382--405},
	title = {Bayesianism vs. Frequentism in Statistical Inference},
	year = 2016}

@article{Sprenger2011-si,
	abstract = {Scientific and statistical inferences build heavily on explicit,
              parametric models, and often with good reasons. However, the
              limited scope of parametric models and the increasing complexity
              of the studied systems in modern science raise the risk of model
              misspecification. Therefore, I examine alternative, data-based
              inference techniques, such as bootstrap resampling. I argue that
              their neglect in the philosophical literature is unjustified:
              they suit some contexts of inquiry much better and use a more
              direct approach to scientific inference. Moreover, they make more
              parsimonious assumptions and often replace theoretical
              understanding and knowledge about mechanisms by careful
              experimental design. Thus, it is worthwhile to study in detail
              how nonparametric models serve as inferential engines in science.},
	author = {Sprenger, Jan},
	date-added = {2024-04-12 09:13:20 -0700},
	date-modified = {2024-04-12 09:13:20 -0700},
	journal = {Synthese},
	month = may,
	number = 1,
	pages = {65--76},
	title = {Science without (parametric) models: the case of bootstrap resampling},
	volume = 180,
	year = 2011}

@book{Sober2015-of,
	abstract = {Ockham's razor, the principle of parsimony, states that simpler
               theories are better than theories that are more complex. It has
               a history dating back to Aristotle and it plays an important
               role in current physics, biology, and psychology. The razor also
               gets used outside of science - in everyday life and in
               philosophy. This book evaluates the principle and discusses its
               many applications. Fascinating examples from different domains
               provide a rich basis for contemplating the principle's promises
               and perils. It is obvious that simpler theories are beautiful
               and easy to understand; the hard problem is to figure out why
               the simplicity of a theory should be relevant to saying what the
               world is like. In this book, the ABCs of probability theory are
               succinctly developed and put to work to describe two 'parsimony
               paradigms' within which this problem can be solved.},
	author = {Sober, Elliott},
	date-added = {2024-04-12 09:13:10 -0700},
	date-modified = {2024-04-12 09:13:10 -0700},
	language = {en},
	month = jul,
	publisher = {Cambridge University Press},
	title = {Ockham's Razors},
	year = 2015}

@book{Savage1972-zh,
	abstract = {Classic analysis of the foundations of statistics and
               development of personal probability, one of the greatest
               controversies in modern statistical thought. Revised edition.
               Calculus, probability, statistics, and Boolean algebra are
               recommended.},
	author = {Savage, Leonard J},
	date-added = {2024-04-12 09:12:58 -0700},
	date-modified = {2024-04-12 09:12:58 -0700},
	language = {en},
	publisher = {Dover},
	title = {The Foundations of Statistics},
	year = 1972}

@book{Mayo1996-re,
	abstract = {We may learn from our mistakes, but Deborah Mayo argues that,
                 where experimental knowledge is concerned, we haven't begun to
                 learn enough. Error and the Growth of Experimental Knowledge
                 launches a vigorous critique of the subjective Bayesian view
                 of statistical inference, and proposes Mayo's own
                 error-statistical approach as a more robust framework for the
                 epistemology of experiment. Mayo genuinely addresses the needs
                 of researchers who work with statistical analysis, and
                 simultaneously engages the basic philosophical problems of
                 objectivity and rationality. Mayo has long argued for an
                 account of learning from error that goes far beyond detecting
                 logical inconsistencies. In this book, she presents her
                 complete program for how we learn about the world by being
                 ``shrewd inquisitors of error, white gloves off.'' Her tough,
                 practical approach will be important to philosophers,
                 historians, and sociologists of science, and will be welcomed
                 by researchers in the physical, biological, and social
                 sciences whose work depends upon statistical analysis.},
	author = {Mayo, Deborah G},
	date-added = {2024-04-12 09:12:49 -0700},
	date-modified = {2024-04-12 09:12:49 -0700},
	language = {en},
	month = aug,
	original_id = {bd9c645b-7342-05c5-b19d-f19761ce76a1},
	publisher = {University of Chicago Press},
	title = {Error and the Growth of Experimental Knowledge},
	year = 1996}

@book{Howson2006-oy,
	abstract = {In this clearly reasoned defense of Bayes's Theorem -- that
               probability can be used to reasonably justify scientific
               theories -- Colin Howson and Peter Urbach examine the way in
               which scientists appeal to probability arguments, and
               demonstrate that the classical approach to statistical inference
               is full of flaws. Arguing the case for the Bayesian method with
               little more than basic algebra, the authors show that it avoids
               the difficulties of the classical system. The book also refutes
               the major criticisms leveled against Bayesian logic, especially
               that it is too subjective. This newly updated edition of this
               classic textbook is also suitable for college courses.},
	author = {Howson, Colin and Urbach, Peter},
	date-added = {2024-04-12 09:12:39 -0700},
	date-modified = {2024-04-12 09:12:39 -0700},
	language = {en},
	publisher = {Open Court Publishing},
	title = {Scientific Reasoning: The Bayesian Approach},
	year = 2006}

@incollection{Hitchcock2009-yj,
	author = {Hitchcock, Christopher},
	booktitle = {The Oxford Handbook of Causation},
	date-added = {2024-04-12 09:12:28 -0700},
	date-modified = {2024-04-12 09:12:28 -0700},
	pages = {299--314},
	title = {Causal Modelling},
	year = 2009}

@article{Hitchcock2001-fv,
	abstract = {WA Te live in exciting times. By'we'I mean philosophers
               study-ing the nature of causation. The past decade or so has
               witnessed a flurry of philosophical activity aimed at crack-ing
               this {\ldots}},
	author = {Hitchcock, Christopher},
	date-added = {2024-04-12 09:12:28 -0700},
	date-modified = {2024-04-12 09:12:28 -0700},
	journal = {J. Philos.},
	number = 6,
	pages = {273--299},
	publisher = {Journal of Philosophy, Inc.},
	title = {The Intransitivity of Causation Revealed in Equations and Graphs},
	volume = 98,
	year = 2001}

@article{Johnson2016-xr,
	abstract = {AbstractFamously, scientific theories are underdetermined by
               their evidence. This occurs in the factor analytic model (FA),
               which is often used to connect concrete data (e.g. test scores)
               to hypothetical notions (e.g. intelligence). After introducing
               FA, three general topics are addressed. (i) Underdetermination:
               the precise reasons why FA is underdetermined illuminates
               various claims about underdetermination, abduction, and
               theoretical terms. (ii) Uncertainties: FA helps distinguish at
               least four kinds of uncertainties. The prevailing practice,
               often encoded in statistical software, is to ignore the most
               difficult kinds, which are essential to FA's underdetermination.
               (iii) What to do: some suggestions for dealing with these
               hardest types of uncertainty are offered.},
	author = {Johnson, Kent},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	date-added = {2024-04-12 09:12:14 -0700},
	date-modified = {2024-04-12 09:12:14 -0700},
	journal = {Nous},
	language = {en},
	month = jun,
	number = 2,
	pages = {329--355},
	publisher = {Wiley},
	title = {Realism and uncertainty of unobservable common causes in factor analysis},
	volume = 50,
	year = 2016}

@book{Jeffreys1948-vt,
	author = {Jeffreys, Harold},
	date-added = {2024-04-12 09:11:58 -0700},
	date-modified = {2024-04-12 09:11:58 -0700},
	language = {en},
	publisher = {Oxford},
	title = {The Theory of Probability},
	year = 1948}

@incollection{Galavotti2014-th,
	author = {Galavotti, Maria Carla},
	booktitle = {The Routledge Companion to Philosophy of Science},
	date-added = {2024-04-12 09:11:46 -0700},
	date-modified = {2024-04-12 09:11:46 -0700},
	edition = {second},
	editor = {Curd, Martin and Psillos, Stathis},
	pages = {458--468},
	title = {Probability},
	year = 2014}

@book{Fisher1990-dp,
	author = {Fisher, Ronald A},
	date-added = {2024-04-12 09:11:34 -0700},
	date-modified = {2024-04-12 09:11:34 -0700},
	publisher = {Oxford University Press},
	title = {Statistical Methods, Experimental Design, and Scientific Inference},
	year = 1990}

@article{Efron1977-vq,
	abstract = {{\ldots} In traditional statistical theory it can be proved that no
               other estimation rule is {\ldots} The paradoxical element in Stein's re
               sult is that it sometimes contradicts this elementary law of
               statistical {\ldots}},
	author = {Efron, Bradley and Morris, Carl},
	date-added = {2024-04-12 09:10:54 -0700},
	date-modified = {2024-04-12 09:10:54 -0700},
	journal = {Sci. Am.},
	number = 5,
	pages = {119--127},
	publisher = {Scientific American, a division of Nature America, Inc.},
	title = {Stein's Paradox in Statistics},
	volume = 236,
	year = 1977}
